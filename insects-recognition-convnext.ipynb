{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9928f1b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-31T22:25:31.976488Z",
     "iopub.status.busy": "2026-01-31T22:25:31.975860Z",
     "iopub.status.idle": "2026-01-31T22:25:49.379382Z",
     "shell.execute_reply": "2026-01-31T22:25:49.378631Z"
    },
    "papermill": {
     "duration": 17.410984,
     "end_time": "2026-01-31T22:25:49.381117",
     "exception": false,
     "start_time": "2026-01-31T22:25:31.970133",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Libreria di sistema\n",
    "import kagglehub\n",
    "from pathlib import Path\n",
    "import os.path\n",
    "\n",
    "# Installa dipendenze base\n",
    "!pip install torch torchvision\n",
    "!pip install pandas numpy matplotlib seaborn\n",
    "!pip install scikit-learn pillow tqdm\n",
    "!pip install jupyter notebook\n",
    "!pip install tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11783f6a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-31T22:25:49.392613Z",
     "iopub.status.busy": "2026-01-31T22:25:49.392340Z",
     "iopub.status.idle": "2026-01-31T22:25:49.508339Z",
     "shell.execute_reply": "2026-01-31T22:25:49.507510Z"
    },
    "papermill": {
     "duration": 0.123454,
     "end_time": "2026-01-31T22:25:49.509855",
     "exception": false,
     "start_time": "2026-01-31T22:25:49.386401",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Download dataset\n",
    "path = kagglehub.dataset_download(\"rtlmhjbn/ip02-dataset\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97ea4d7a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-31T22:25:49.519883Z",
     "iopub.status.busy": "2026-01-31T22:25:49.519653Z",
     "iopub.status.idle": "2026-01-31T22:25:51.855859Z",
     "shell.execute_reply": "2026-01-31T22:25:51.855276Z"
    },
    "papermill": {
     "duration": 2.343051,
     "end_time": "2026-01-31T22:25:51.857540",
     "exception": false,
     "start_time": "2026-01-31T22:25:49.514489",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from collections import Counter\n",
    "import json\n",
    "\n",
    "# Configurazione visualizzazioni\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8747d6ff",
   "metadata": {},
   "source": [
    "# Analisi Distribuzioni Classi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e4079f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-31T22:25:51.868236Z",
     "iopub.status.busy": "2026-01-31T22:25:51.867629Z",
     "iopub.status.idle": "2026-01-31T22:25:54.614513Z",
     "shell.execute_reply": "2026-01-31T22:25:54.613600Z"
    },
    "papermill": {
     "duration": 2.753763,
     "end_time": "2026-01-31T22:25:54.615973",
     "exception": false,
     "start_time": "2026-01-31T22:25:51.862210",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATA_DIR = Path(path) / \"classification\"\n",
    "TRAIN_DIR = DATA_DIR / \"train\"\n",
    "VAL_DIR = DATA_DIR / \"val\"\n",
    "TEST_DIR = DATA_DIR / \"test\"\n",
    "\n",
    "print(f\"Train directory exists: {TRAIN_DIR.exists()}\")\n",
    "print(f\"Val directory exists: {VAL_DIR.exists()}\")\n",
    "print(f\"Test directory exists: {TEST_DIR.exists()}\")\n",
    "\n",
    "\n",
    "# ANALISI DISTRIBUZIONE CLASSI\n",
    "\n",
    "def count_images_per_class(data_dir):\n",
    "    # Conta immagini per ogni classe\n",
    "    class_counts = {}\n",
    "    \n",
    "    for class_folder in sorted(data_dir.iterdir()):\n",
    "        if class_folder.is_dir():\n",
    "            class_name = class_folder.name\n",
    "            num_images = len(list(class_folder.glob('*.jpg'))) + \\\n",
    "                        len(list(class_folder.glob('*.png'))) + \\\n",
    "                        len(list(class_folder.glob('*.jpeg')))\n",
    "            class_counts[class_name] = num_images\n",
    "    \n",
    "    return class_counts\n",
    "\n",
    "# Conta immagini per split\n",
    "train_counts = count_images_per_class(TRAIN_DIR)\n",
    "val_counts = count_images_per_class(VAL_DIR) if VAL_DIR.exists() else {}\n",
    "test_counts = count_images_per_class(TEST_DIR) if TEST_DIR.exists() else {}\n",
    "\n",
    "# Crea DataFrame\n",
    "df_train = pd.DataFrame(list(train_counts.items()), columns=['Class', 'Train_Count'])\n",
    "if val_counts:\n",
    "    df_val = pd.DataFrame(list(val_counts.items()), columns=['Class', 'Val_Count'])\n",
    "    df_train = df_train.merge(df_val, on='Class', how='outer')\n",
    "if test_counts:\n",
    "    df_test = pd.DataFrame(list(test_counts.items()), columns=['Class', 'Test_Count'])\n",
    "    df_train = df_train.merge(df_test, on='Class', how='outer')\n",
    "\n",
    "df_train = df_train.fillna(0).astype({'Train_Count': int})\n",
    "\n",
    "\n",
    "print(\"STATISTICHE DATASET\")\n",
    "print(f\"Numero totale di classi: {len(train_counts)}\")\n",
    "print(f\"Totale immagini train: {sum(train_counts.values())}\")\n",
    "if val_counts:\n",
    "    print(f\"Totale immagini val: {sum(val_counts.values())}\")\n",
    "if test_counts:\n",
    "    print(f\"Totale immagini test: {sum(test_counts.values())}\")\n",
    "\n",
    "\n",
    "print(\"STATISTICHE TRAIN SET\")\n",
    "print(f\"Media immagini per classe: {np.mean(list(train_counts.values())):.1f}\")\n",
    "print(f\"Mediana: {np.median(list(train_counts.values())):.1f}\")\n",
    "print(f\"Min immagini: {min(train_counts.values())} (classe: {min(train_counts, key=train_counts.get)})\")\n",
    "print(f\"Max immagini: {max(train_counts.values())} (classe: {max(train_counts, key=train_counts.get)})\")\n",
    "print(f\"Std deviation: {np.std(list(train_counts.values())):.1f}\")\n",
    "\n",
    "# Calcola imbalance ratio\n",
    "imbalance_ratio = max(train_counts.values()) / min(train_counts.values())\n",
    "print(f\"\\n IMBALANCE RATIO: {imbalance_ratio:.2f}x\")\n",
    "if imbalance_ratio > 10:\n",
    "    print(\"SEVERAMENTE SBILANCIATO - Necessarie tecniche di bilanciamento!\")\n",
    "elif imbalance_ratio > 5:\n",
    "    print(\"MODERATAMENTE SBILANCIATO - Consigliato class weighting\")\n",
    "else:\n",
    "    print(\"RELATIVAMENTE BILANCIATO\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dda93ae",
   "metadata": {},
   "source": [
    "# Visualizzazione Distribuzione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f74d80",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-31T22:25:54.627116Z",
     "iopub.status.busy": "2026-01-31T22:25:54.626853Z",
     "iopub.status.idle": "2026-01-31T22:25:55.209447Z",
     "shell.execute_reply": "2026-01-31T22:25:55.208594Z"
    },
    "papermill": {
     "duration": 0.591136,
     "end_time": "2026-01-31T22:25:55.212053",
     "exception": false,
     "start_time": "2026-01-31T22:25:54.620917",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 1, figsize=(16, 12))\n",
    "\n",
    "# Plot 1: Distribuzione ordinata\n",
    "sorted_counts = dict(sorted(train_counts.items(), key=lambda x: x[1], reverse=True))\n",
    "axes[0].bar(range(len(sorted_counts)), list(sorted_counts.values()), color='steelblue', alpha=0.7)\n",
    "axes[0].axhline(np.mean(list(train_counts.values())), color='red', linestyle='--', \n",
    "                label=f'Media: {np.mean(list(train_counts.values())):.0f}', linewidth=2)\n",
    "axes[0].set_xlabel('Classe (ordinata per frequenza)', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('Numero di Immagini', fontsize=12, fontweight='bold')\n",
    "axes[0].set_title('Distribuzione Classi - Train Set (Ordinato)', fontsize=14, fontweight='bold')\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Plot 2: Istogramma distribuzione\n",
    "axes[1].hist(list(train_counts.values()), bins=30, color='coral', alpha=0.7, edgecolor='black')\n",
    "axes[1].axvline(np.mean(list(train_counts.values())), color='red', linestyle='--', \n",
    "                label=f'Media: {np.mean(list(train_counts.values())):.0f}', linewidth=2)\n",
    "axes[1].axvline(np.median(list(train_counts.values())), color='green', linestyle='--', \n",
    "                label=f'Mediana: {np.median(list(train_counts.values())):.0f}', linewidth=2)\n",
    "axes[1].set_xlabel('Numero di Immagini per Classe', fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylabel('Frequenza (numero di classi)', fontsize=12, fontweight='bold')\n",
    "axes[1].set_title('Distribuzione della Frequenza delle Classi', fontsize=14, fontweight='bold')\n",
    "axes[1].legend(fontsize=11)\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Identifica classi problematiche\n",
    "threshold_low = np.percentile(list(train_counts.values()), 25)\n",
    "threshold_high = np.percentile(list(train_counts.values()), 75)\n",
    "\n",
    "underrepresented = {k: v for k, v in train_counts.items() if v < threshold_low}\n",
    "overrepresented = {k: v for k, v in train_counts.items() if v > threshold_high}\n",
    "\n",
    "print(f\"\\nüìä Classi sotto-rappresentate (<25¬∞ percentile = {threshold_low:.0f} img): {len(underrepresented)}\")\n",
    "print(f\"üìä Classi sovra-rappresentate (>75¬∞ percentile = {threshold_high:.0f} img): {len(overrepresented)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf08c3d",
   "metadata": {},
   "source": [
    "# Visualizzazione Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e534fb70",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-31T22:26:00.532468Z",
     "iopub.status.busy": "2026-01-31T22:26:00.531900Z",
     "iopub.status.idle": "2026-01-31T22:26:02.206323Z",
     "shell.execute_reply": "2026-01-31T22:26:02.205552Z"
    },
    "papermill": {
     "duration": 1.742365,
     "end_time": "2026-01-31T22:26:02.266832",
     "exception": false,
     "start_time": "2026-01-31T22:26:00.524467",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def show_sample_images(data_dir, n_classes=6, n_images_per_class=5):\n",
    "    # Mostra immagini di esempio da diverse classi\n",
    "    classes = sorted([d.name for d in data_dir.iterdir() if d.is_dir()])\n",
    "    selected_classes = np.random.choice(classes, min(n_classes, len(classes)), replace=False)\n",
    "    \n",
    "    fig, axes = plt.subplots(n_classes, n_images_per_class, \n",
    "                             figsize=(3*n_images_per_class, 3*n_classes))\n",
    "    fig.suptitle('Esempi di Immagini dal Dataset', fontsize=16, fontweight='bold', y=0.995)\n",
    "    \n",
    "    for i, class_name in enumerate(selected_classes):\n",
    "        class_dir = data_dir / class_name\n",
    "        images = list(class_dir.glob('*.jpg')) + list(class_dir.glob('*.png'))\n",
    "        selected_images = np.random.choice(images, min(n_images_per_class, len(images)), replace=False)\n",
    "        \n",
    "        for j, img_path in enumerate(selected_images):\n",
    "            img = Image.open(img_path)\n",
    "            axes[i, j].imshow(img)\n",
    "            axes[i, j].axis('off')\n",
    "            if j == 0:\n",
    "                axes[i, j].set_ylabel(f'{class_name}\\n({train_counts[class_name]} img)', \n",
    "                                     fontsize=10, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "show_sample_images(TRAIN_DIR, n_classes=6, n_images_per_class=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae6745e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-31T22:26:02.382624Z",
     "iopub.status.busy": "2026-01-31T22:26:02.382297Z",
     "iopub.status.idle": "2026-01-31T22:26:02.392724Z",
     "shell.execute_reply": "2026-01-31T22:26:02.392048Z"
    },
    "papermill": {
     "duration": 0.070527,
     "end_time": "2026-01-31T22:26:02.395001",
     "exception": false,
     "start_time": "2026-01-31T22:26:02.324474",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_class_weights(class_counts):\n",
    "    # Calcola pesi per bilanciare le classi\n",
    "    total = sum(class_counts.values())\n",
    "    n_classes = len(class_counts)\n",
    "    \n",
    "    # Formula: weight = total / (n_classes * class_count)\n",
    "    weights = {cls: total / (n_classes * count) for cls, count in class_counts.items()}\n",
    "    \n",
    "    return weights\n",
    "\n",
    "class_weights = calculate_class_weights(train_counts)\n",
    "\n",
    "# Visualizza top-10 pesi\n",
    "sorted_weights = dict(sorted(class_weights.items(), key=lambda x: x[1], reverse=True)[:50])\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TOP 10 CLASS WEIGHTS (per loss function)\")\n",
    "print(\"=\"*80)\n",
    "for cls, weight in sorted_weights.items():\n",
    "    print(f\"{cls:30s}: {weight:.4f} (n_images={train_counts[cls]})\")\n",
    "\n",
    "# Salva per uso futuro\n",
    "weights_dict = {\n",
    "    'class_weights': class_weights,\n",
    "    'class_counts': train_counts\n",
    "}\n",
    "\n",
    "print(\" EDA COMPLETATA - Prossimo step: implementare data loader e baseline model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ccc075f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-31T22:26:02.512350Z",
     "iopub.status.busy": "2026-01-31T22:26:02.511817Z",
     "iopub.status.idle": "2026-01-31T22:26:05.611091Z",
     "shell.execute_reply": "2026-01-31T22:26:05.610063Z"
    },
    "papermill": {
     "duration": 3.158752,
     "end_time": "2026-01-31T22:26:05.612764",
     "exception": false,
     "start_time": "2026-01-31T22:26:02.454012",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install albumentations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252b629a",
   "metadata": {},
   "source": [
    "# Preparazione Dataset e Bilanciamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2b1e6d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-31T22:26:05.733689Z",
     "iopub.status.busy": "2026-01-31T22:26:05.733383Z",
     "iopub.status.idle": "2026-01-31T22:26:16.314912Z",
     "shell.execute_reply": "2026-01-31T22:26:16.314296Z"
    },
    "papermill": {
     "duration": 10.643185,
     "end_time": "2026-01-31T22:26:16.316544",
     "exception": false,
     "start_time": "2026-01-31T22:26:05.673359",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "import json\n",
    "import numpy as np\n",
    "from typing import Optional, Dict, Tuple\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "class IP102Dataset(Dataset):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        root_dir: str,\n",
    "        transform=None,\n",
    "        resize_limit=1024,\n",
    "        augment: bool = True,\n",
    "        cache_images: bool = False\n",
    "    ):\n",
    "        self.root_dir = Path(root_dir)\n",
    "        self.transform = transform\n",
    "        self.augment = augment\n",
    "        self.cache_images = cache_images\n",
    "        self.resize_limit = resize_limit\n",
    "        \n",
    "        # Carica immagini e labels\n",
    "        self.samples = []\n",
    "        self.class_to_idx = {}\n",
    "        self.idx_to_class = {}\n",
    "        self._load_dataset()\n",
    "        \n",
    "        # Cache per immagini (opzionale)\n",
    "        self.image_cache = {} if cache_images else None\n",
    "        \n",
    "        print(f\" Dataset caricato: {len(self.samples)} immagini, {len(self.class_to_idx)} classi\")\n",
    "    \n",
    "    def _load_dataset(self):\n",
    "        # Carica percorsi immagini e crea mappatura classi\n",
    "        classes = sorted([d.name for d in self.root_dir.iterdir() if d.is_dir()])\n",
    "        self.class_to_idx = {cls_name: idx for idx, cls_name in enumerate(classes)}\n",
    "        self.idx_to_class = {idx: cls_name for cls_name, idx in self.class_to_idx.items()}\n",
    "        \n",
    "        for class_name in classes:\n",
    "            class_dir = self.root_dir / class_name\n",
    "            class_idx = self.class_to_idx[class_name]\n",
    "            \n",
    "            # Trova tutte le immagini\n",
    "            for ext in ['*.jpg', '*.jpeg', '*.png', '*.JPG', '*.JPEG', '*.PNG']:\n",
    "                for img_path in class_dir.glob(ext):\n",
    "                    self.samples.append((str(img_path), class_idx))\n",
    "    \n",
    "    def get_class_distribution(self) -> Dict[int, int]:\n",
    "        # Restituisce distribuzione delle classi\n",
    "        class_counts = {}\n",
    "        for _, label in self.samples:\n",
    "            class_counts[label] = class_counts.get(label, 0) + 1\n",
    "        return class_counts\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def _fast_load(self, path):\n",
    "        img = cv2.imread(path, cv2.IMREAD_COLOR)\n",
    "\n",
    "        # in caso di errore di lettura\n",
    "        if img is None:\n",
    "            return np.zeros((224, 224, 3), dtype=np.uint8)\n",
    "\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # immagini IP102 molto grandi ‚Üí riduzione preventiva\n",
    "        h, w = img.shape[:2]\n",
    "        max_dim = max(h, w)\n",
    "        if max_dim > self.resize_limit:\n",
    "            scale = self.resize_limit / max_dim\n",
    "            img = cv2.resize(img, (int(w * scale), int(h * scale)))\n",
    "\n",
    "        return img\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.samples[idx]\n",
    "        img = self._fast_load(img_path)\n",
    "        \n",
    "        # Applica trasformazioni\n",
    "        if self.transform:\n",
    "            img = self.transform(image=img)[\"image\"]\n",
    "        \n",
    "        return img, label\n",
    "\n",
    "def get_train_transforms(img_size: int = 224):\n",
    "   \n",
    "    return A.Compose([\n",
    "\n",
    "        A.RandomResizedCrop(size=(img_size, img_size),scale=(0.7, 1.0),ratio=(0.75, 1.33),p=1.0),\n",
    "        \n",
    "        # Augmentazioni leggere e realistiche\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.VerticalFlip(p=0.25),\n",
    "\n",
    "        A.Rotate(limit=25, p=0.5),  # non distrugge la forma dell'insetto\n",
    "        \n",
    "        A.ColorJitter(\n",
    "            brightness=0.2, contrast=0.2, saturation=0.2, hue=0.05,\n",
    "            p=0.5\n",
    "        ),\n",
    "\n",
    "        # leggerissimo rumore\n",
    "        A.GaussNoise(var_limit=(5.0, 15.0), p=0.15),\n",
    "\n",
    "        # piccolo CutOut\n",
    "        A.CoarseDropout(\n",
    "            max_holes=6,\n",
    "            max_height=20,\n",
    "            max_width=20,\n",
    "            min_holes=1,\n",
    "            fill_value=0,\n",
    "            p=0.2\n",
    "        ),\n",
    "\n",
    "        A.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406],\n",
    "            std=[0.229, 0.224, 0.225]\n",
    "        ),\n",
    "        ToTensorV2()\n",
    "    ])\n",
    "    \n",
    "\n",
    "def get_val_transforms(img_size=224):\n",
    "    return A.Compose([\n",
    "        A.Resize(img_size, img_size),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2()\n",
    "    ])\n",
    "\n",
    "\n",
    "\n",
    "def create_weighted_sampler(dataset: IP102Dataset) -> WeightedRandomSampler:\n",
    "    class_counts = dataset.get_class_distribution()\n",
    "    \n",
    "    # Calcola pesi: inversamente proporzionale alla frequenza\n",
    "    total_samples = len(dataset)\n",
    "    num_classes = len(class_counts)\n",
    "    class_weights = {\n",
    "        cls: total_samples / (num_classes * count) \n",
    "        for cls, count in class_counts.items()\n",
    "    }\n",
    "    \n",
    "    # Assegna peso a ogni sample\n",
    "    sample_weights = [class_weights[label] for _, label in dataset.samples]\n",
    "    \n",
    "    sampler = WeightedRandomSampler(\n",
    "        weights=sample_weights,\n",
    "        num_samples=len(sample_weights),\n",
    "        replacement=True\n",
    "    )\n",
    "    \n",
    "    return sampler\n",
    "\n",
    "\n",
    "def get_dataloaders(\n",
    "    train_dir: str,\n",
    "    val_dir: str,\n",
    "    batch_size: int = 32,\n",
    "    img_size: int = 224,\n",
    "    num_workers: int = 4\n",
    "):\n",
    "    \n",
    "    # Crea transforms\n",
    "    train_transform = get_train_transforms(img_size)\n",
    "    val_transform = get_val_transforms(img_size)\n",
    "    \n",
    "    # Crea datasets\n",
    "    train_dataset = IP102Dataset(train_dir, transform=train_transform)\n",
    "    val_dataset = IP102Dataset(val_dir, transform=val_transform)\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "        drop_last=True  # Drop incomplete batch per stabilit√† training\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úî Train samples: {len(train_dataset)}\")\n",
    "    print(f\"‚úî Val samples:   {len(val_dataset)}\")\n",
    "    \n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ab5a45",
   "metadata": {},
   "source": [
    "# Visualizzazione immagini dal nuovo DataLoader IP102"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693a2bed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-31T22:26:16.435875Z",
     "iopub.status.busy": "2026-01-31T22:26:16.434994Z",
     "iopub.status.idle": "2026-01-31T22:26:18.968211Z",
     "shell.execute_reply": "2026-01-31T22:26:18.967309Z"
    },
    "papermill": {
     "duration": 2.631967,
     "end_time": "2026-01-31T22:26:19.008714",
     "exception": false,
     "start_time": "2026-01-31T22:26:16.376747",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def denormalize(img_tensor):\n",
    "    img = img_tensor.cpu().numpy().transpose(1, 2, 0)\n",
    "\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std  = np.array([0.229, 0.224, 0.225])\n",
    "\n",
    "    img = img * std + mean\n",
    "    img = np.clip(img, 0, 1)\n",
    "    return img\n",
    "\n",
    "TRAIN_DIR = path + '/classification/train'\n",
    "VAL_DIR   = path + '/classification/val'\n",
    "\n",
    "train_loader, val_loader = get_dataloaders(\n",
    "    train_dir=TRAIN_DIR,\n",
    "    val_dir=VAL_DIR,\n",
    "    batch_size=16,\n",
    "    img_size=224,\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "print(\"\\n Carico un batch dal DataLoader‚Ä¶\\n\")\n",
    "images, labels = next(iter(train_loader))\n",
    "\n",
    "print(\" Batch images:\", images.shape)\n",
    "print(\" Batch labels:\", labels.shape)\n",
    "print(\" Example labels:\", labels[:10].tolist())\n",
    "\n",
    "plt.figure(figsize=(14, 10))\n",
    "plt.suptitle(\"Esempi di immagini augmentate (train_loader)\", fontsize=16, y=1.02)\n",
    "\n",
    "for i in range(12):  # mostra 12 immagini\n",
    "    img = denormalize(images[i])\n",
    "\n",
    "    plt.subplot(3, 4, i+1)\n",
    "    plt.imshow(img)\n",
    "    plt.title(f\"Class {labels[i].item()}\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30feaeb1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-31T22:26:19.196617Z",
     "iopub.status.busy": "2026-01-31T22:26:19.195913Z",
     "iopub.status.idle": "2026-01-31T22:26:22.394657Z",
     "shell.execute_reply": "2026-01-31T22:26:22.393794Z"
    },
    "papermill": {
     "duration": 3.293646,
     "end_time": "2026-01-31T22:26:22.396278",
     "exception": false,
     "start_time": "2026-01-31T22:26:19.102632",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a845bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-31T22:26:22.589480Z",
     "iopub.status.busy": "2026-01-31T22:26:22.588774Z",
     "iopub.status.idle": "2026-01-31T22:26:22.595133Z",
     "shell.execute_reply": "2026-01-31T22:26:22.594472Z"
    },
    "papermill": {
     "duration": 0.104813,
     "end_time": "2026-01-31T22:26:22.596505",
     "exception": false,
     "start_time": "2026-01-31T22:26:22.491692",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "path_checkpoints = Path(\"/kaggle/working/checkpoints\")\n",
    "path_logs = Path(\"/kaggle/working/logs\")\n",
    "\n",
    "path_checkpoints.mkdir(parents=True, exist_ok=True)\n",
    "path_logs.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if path_checkpoints.exists() and path_checkpoints.is_dir() and path_logs.exists() and path_logs.is_dir():\n",
    "    print(\"Cartelle trovate\")\n",
    "else:\n",
    "    print(\"Cartelle non trovate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f4344a",
   "metadata": {},
   "source": [
    "# Esplorazione Modelli ConvNeXt (timm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25aacf63",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-31T22:26:22.787170Z",
     "iopub.status.busy": "2026-01-31T22:26:22.786438Z",
     "iopub.status.idle": "2026-01-31T22:26:26.220624Z",
     "shell.execute_reply": "2026-01-31T22:26:26.219412Z"
    },
    "papermill": {
     "duration": 3.531591,
     "end_time": "2026-01-31T22:26:26.222912",
     "exception": false,
     "start_time": "2026-01-31T22:26:22.691321",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import timm\n",
    "# Lista tutti i modelli ConvNext disponibili\n",
    "convnext_models = timm.list_models('*convnext*', pretrained=True)\n",
    "print(\"ConvNext models con pretrained weights:\")\n",
    "for model in convnext_models[:50]:  # Mostra primi 20\n",
    "    print(f\"  - {model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110a0038",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-31T22:26:26.412204Z",
     "iopub.status.busy": "2026-01-31T22:26:26.411880Z",
     "iopub.status.idle": "2026-01-31T22:26:29.713369Z",
     "shell.execute_reply": "2026-01-31T22:26:29.712406Z"
    },
    "papermill": {
     "duration": 3.395808,
     "end_time": "2026-01-31T22:26:29.714928",
     "exception": false,
     "start_time": "2026-01-31T22:26:26.319120",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import timm\n",
    "m = timm.create_model('convnext_base.fb_in22k_ft_in1k', pretrained=True)\n",
    "print(m.default_cfg['input_size'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa49af9",
   "metadata": {},
   "source": [
    "# Training Pipeline e Ottimizzazione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6d97b4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-31T22:27:10.187846Z",
     "iopub.status.busy": "2026-01-31T22:27:10.187369Z",
     "iopub.status.idle": "2026-02-01T07:50:31.325590Z",
     "shell.execute_reply": "2026-02-01T07:50:31.324703Z"
    },
    "papermill": {
     "duration": 33801.258609,
     "end_time": "2026-02-01T07:50:31.353392",
     "exception": false,
     "start_time": "2026-01-31T22:27:10.094783",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, OneCycleLR\n",
    "import timm\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, balanced_accuracy_score, \n",
    "    f1_score, confusion_matrix\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import seaborn as sns\n",
    "import json\n",
    "from copy import deepcopy\n",
    "from pathlib import Path\n",
    "import time\n",
    "import random\n",
    "from typing import Dict, Tuple, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torchvision.transforms as T\n",
    "import torchvision.datasets as datasets \n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "\n",
    "def mixup_data(x: torch.Tensor, y: torch.LongTensor, alpha: float = 0.3):\n",
    "    \n",
    "    if alpha > 0.:\n",
    "        lam = float(np.random.beta(alpha, alpha))\n",
    "    else:\n",
    "        lam = 1\n",
    "        \n",
    "    batch_size = x.size()[0]\n",
    "    index = torch.randperm(batch_size, device=x.device)\n",
    "    \n",
    "    mixed_x = lam * x + (1 - lam) * x[index]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "\n",
    "def cutmix_data(x: torch.Tensor, y: torch.LongTensor, alpha: float = 0.5):\n",
    "    \n",
    "    if alpha > 0.:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1.0\n",
    "\n",
    "    \n",
    "    batch_size, _, W, H = x.size()\n",
    "    index = torch.randperm(batch_size, device=x.device)\n",
    "\n",
    "    cut_rat = np.sqrt(1. - lam)\n",
    "    cut_w = int(W * cut_rat)\n",
    "    cut_h = int(H * cut_rat)\n",
    "\n",
    "    cx = np.random.randint(W)\n",
    "    cy = np.random.randint(H)\n",
    "\n",
    "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "\n",
    "    x[:, :, bbx1:bbx2, bby1:bby2] = x[index, :, bbx1:bbx2, bby1:bby2]\n",
    "    lam = 1.0 - ((bbx2 - bbx1) * (bby2 - bby1) / (W * H))\n",
    "\n",
    "    y_a, y_b = y, y[index]\n",
    "    return x, y_a, y_b, lam\n",
    "\n",
    "\n",
    "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
    "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n",
    "\n",
    "def load_last_checkpoint(trainer, checkpoint_path, device, config):\n",
    "    if not checkpoint_path.exists():\n",
    "        print(\"Nessun checkpoint trovato, training da zero\")\n",
    "        return 1\n",
    "\n",
    "    print(f\" Ripristino checkpoint: {checkpoint_path}\")\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)\n",
    "\n",
    "    trainer.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    try:\n",
    "        trainer.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        for state in trainer.optimizer.state.values():\n",
    "            for k, v in state.items():\n",
    "                if isinstance(v, torch.Tensor):\n",
    "                    state[k] = v.to(device)\n",
    "    except Exception as e:\n",
    "        print(f\" Optimizer non ripristinato: {e}\")\n",
    "\n",
    "    start_epoch = checkpoint['epoch'] + 1\n",
    "  \n",
    "    sch_state = checkpoint.get('scheduler_state_dict', None)\n",
    "\n",
    "    if trainer.scheduler is not None and sch_state is not None:\n",
    "        trainer.scheduler.load_state_dict(sch_state)\n",
    "\n",
    "        ckpt_pgs = checkpoint['optimizer_state_dict']['param_groups']\n",
    "        if len(ckpt_pgs) == len(trainer.optimizer.param_groups):\n",
    "            for i, pg in enumerate(ckpt_pgs):\n",
    "                trainer.optimizer.param_groups[i]['lr'] = pg['lr']\n",
    "            print(f\" Scheduler ripristinato | LR: {trainer.optimizer.param_groups[0]['lr']:.8e}\")\n",
    "        else:\n",
    "            print(f\" param_groups non allineati: checkpoint={len(ckpt_pgs)}, optimizer={len(trainer.optimizer.param_groups)}\")\n",
    "\n",
    "\n",
    "    if trainer.use_amp and checkpoint.get('scaler_state') is not None:\n",
    "        trainer.scaler.load_state_dict(checkpoint['scaler_state'])\n",
    "\n",
    "    if trainer.use_ema and trainer.ema is not None and checkpoint.get('ema_state') is not None:\n",
    "        trainer.ema.shadow = checkpoint['ema_state']\n",
    "\n",
    "    trainer.best_val_acc = checkpoint.get('best_val_acc', 0.0)\n",
    "    trainer.best_balanced_acc = checkpoint.get('best_balanced_acc', 0.0)\n",
    "    trainer.history = checkpoint.get('history', trainer.history)\n",
    "    trainer._patience_counter = checkpoint.get('patience_counter', 0)\n",
    "    \n",
    "    print(f\"Ripristinato da epoch {checkpoint['epoch']}\")\n",
    "    print(f\"Scheduler ripristinato dallo stato salvato)\")\n",
    "\n",
    "    return start_epoch\n",
    "\n",
    "class EMA:\n",
    "    def __init__(self, model: nn.Module, decay: float = 0.9999):\n",
    "        self.decay = decay\n",
    "        self.model = model\n",
    "        self.shadow = {}\n",
    "        self.backup = {}\n",
    "        \n",
    "        for name, param in model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                self.shadow[name] = param.data.clone()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def update(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if not param.requires_grad:\n",
    "                continue\n",
    "            new_avg = self.decay * self.shadow[name] + (1.0 - self.decay) * param.data\n",
    "            self.shadow[name] = new_avg.clone()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def apply_shadow(self):\n",
    "        self.backup = {}\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                self.backup[name] = param.data.clone()\n",
    "                param.data = self.shadow[name].clone()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def restore(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                param.data = self.backup[name].clone()\n",
    "        self.backup = {}\n",
    "\n",
    "\n",
    "\n",
    "def linear_combination(x, y, epsilon):\n",
    "    return epsilon * x + (1 - epsilon) * y\n",
    "\n",
    "def reduce_loss(loss, reduction='mean'):\n",
    "    return loss.mean() if reduction == 'mean' else loss.sum() if reduction == 'sum' else loss\n",
    "\n",
    "\n",
    "class LabelSmoothingCrossEntropy(nn.Module):\n",
    "    def __init__(self, epsilon: float = 0.1, reduction='mean'):\n",
    "        super().__init__()\n",
    "        self.epsilon = epsilon\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, preds, target):\n",
    "        n = preds.size()[-1]\n",
    "        log_preds = F.log_softmax(preds, dim=-1)\n",
    "        loss = reduce_loss(-log_preds.sum(dim=-1), self.reduction)\n",
    "        nll = F.nll_loss(log_preds, target, reduction=self.reduction)\n",
    "        return linear_combination(loss / n, nll, self.epsilon)\n",
    "\n",
    "\n",
    "\n",
    "class IP102Classifier(nn.Module):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str = 'convnext_base.fb_in22k_ft_in1k',\n",
    "        num_classes: int = 102,\n",
    "        pretrained: bool = True,\n",
    "        dropout: float = 0.2, \n",
    "        drop_path_rate: float = 0.2\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Carica backbone con stochastic depth\n",
    "        try:\n",
    "            self.backbone = timm.create_model(\n",
    "            model_name,\n",
    "            pretrained=pretrained,\n",
    "            num_classes=0,\n",
    "            global_pool='avg',\n",
    "            drop_path_rate=drop_path_rate\n",
    "            )\n",
    "        except:\n",
    "            print(\"drop_path_rate non supportato\")\n",
    "            self.backbone = timm.create_model(\n",
    "            model_name,\n",
    "            pretrained=pretrained,\n",
    "            num_classes=0,\n",
    "            global_pool='avg'\n",
    "        )\n",
    "        \n",
    "        # get number of features\n",
    "        num_features = getattr(self.backbone, 'num_features', None)\n",
    "\n",
    "        if num_features is None:\n",
    "            with torch.no_grad():\n",
    "                dummy = torch.randn(1, 3, 224, 224)\n",
    "                out = self.backbone(dummy)\n",
    "                num_features = out.shape[1]        \n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.LayerNorm(num_features),\n",
    "            nn.Linear(num_features, num_classes)\n",
    "        )\n",
    "        \n",
    "        print(f\" Modello: {model_name}\")\n",
    "        print(f\"  Features: {num_features}, Dropout: {dropout}, Drop Path: {drop_path_rate}\")\n",
    "    \n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)\n",
    "        out = self.classifier(features)\n",
    "        return out\n",
    "    \n",
    "    def freeze_backbone(self):\n",
    "        # Congela backbone per fine-tuning graduale\n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = False\n",
    "        print(\"Backbone congelato\")\n",
    "    \n",
    "    def unfreeze_backbone(self):\n",
    "        # Scongela backbone\n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = True\n",
    "        print(\"Backbone scongelato\")\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model: nn.Module,\n",
    "        train_loader : DataLoader,\n",
    "        val_loader: DataLoader,\n",
    "        criterion,\n",
    "        optimizer,\n",
    "        scheduler,\n",
    "        accum_steps=1,\n",
    "        device: str = 'cuda',\n",
    "        save_dir: str = './checkpoints',\n",
    "        log_dir: str = './logs',\n",
    "        use_mixup: bool = False,\n",
    "        use_cutmix: bool = True,\n",
    "        use_amp: bool = True,\n",
    "        use_ema: bool = True,\n",
    "        ema_decay: float = 0.999,\n",
    "        mixup_prob: float = 0.5,\n",
    "        mixup_alpha: float = 0.4,\n",
    "        cutmix_alpha: float = 1.0,\n",
    "        grad_clip: float = 5.0,\n",
    "        config: dict = None,\n",
    "    ):\n",
    "        self.model = model.to(device)\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "        self.accum_steps = accum_steps\n",
    "        self.device = device\n",
    "        \n",
    "        # NUOVO: Mixup/CutMix config\n",
    "        self.use_mixup = use_mixup\n",
    "        self.use_cutmix = use_cutmix\n",
    "        self.mixup_prob = mixup_prob\n",
    "        self.mixup_alpha = mixup_alpha\n",
    "        self.cutmix_alpha = cutmix_alpha\n",
    "        self.grad_clip = grad_clip\n",
    "\n",
    "        self.use_amp = use_amp\n",
    "        self.scaler = GradScaler()\n",
    "        \n",
    "        self.use_ema = use_ema\n",
    "        self.ema = EMA(self.model, decay=ema_decay) if self.use_ema else None\n",
    "        \n",
    "        self.save_dir = Path(save_dir)\n",
    "        self.log_dir = Path(log_dir)\n",
    "        self.save_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.log_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        self.history = {\n",
    "            'train_loss': [], 'train_acc': [],\n",
    "            'val_loss': [], 'val_acc': [],\n",
    "            'val_balanced_acc': [], 'val_f1': [],\n",
    "            'test_loss': None, 'test_acc': None,\n",
    "            'test_balanced_acc': None, 'test_f1': None,\n",
    "            'lr': [],\n",
    "        }\n",
    "        \n",
    "        self.best_val_acc = 0.0\n",
    "        self.best_balanced_acc = 0.0\n",
    "        self._patience_counter = 0\n",
    "        \n",
    "        self.config = config if config is not None else {}\n",
    "\n",
    "\n",
    "    def tta_forward(self, images):\n",
    "\n",
    "        flips = [\n",
    "            images,\n",
    "            torch.flip(images, dims=[3])\n",
    "        ]\n",
    "\n",
    "        logits_sum = 0\n",
    "\n",
    "        for f in flips:\n",
    "            with autocast(enabled=self.use_amp):\n",
    "                out = self.model(f.float())\n",
    "            logits_sum += out    \n",
    "\n",
    "        return logits_sum / len(flips)\n",
    "\n",
    "    \n",
    "    \n",
    "    def train_epoch(self, epoch: int) -> Tuple[float, Optional[float]]:\n",
    "        \n",
    "        self.model.train()\n",
    "\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        running_loss = 0.0\n",
    "\n",
    "        clean_running_loss= 0.0\n",
    "        clean_samples = 0\n",
    "        \n",
    "        pbar = tqdm(self.train_loader, desc=f'Epoch {epoch} [TRAIN]')\n",
    "\n",
    "        \n",
    "        for batch_idx, (images, labels) in enumerate(pbar):\n",
    "            images = images.to(self.device, non_blocking=True)\n",
    "            labels = labels.to(self.device, non_blocking=True)\n",
    "\n",
    "            will_mix = (random.random() < self.mixup_prob) and (self.use_mixup or self.use_cutmix)\n",
    "            \n",
    "            if will_mix:\n",
    "                if self.use_mixup and self.use_cutmix:\n",
    "                    if random.random() < 0.5:\n",
    "                        images, labels_a, labels_b, lam = mixup_data(images, labels, alpha=self.mixup_alpha)\n",
    "                    else:\n",
    "                        images, labels_a, labels_b, lam = cutmix_data(images, labels, alpha=self.cutmix_alpha)\n",
    "                elif self.use_mixup:\n",
    "                    images, labels_a, labels_b, lam = mixup_data(images, labels, alpha=self.mixup_alpha)\n",
    "                else:\n",
    "                    images, labels_a, labels_b, lam = cutmix_data(images, labels, alpha=self.cutmix_alpha)\n",
    "\n",
    "            if (batch_idx % self.accum_steps) == 0:\n",
    "                self.optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            with autocast(enabled=self.use_amp):\n",
    "                outputs = self.model(images)\n",
    "                if will_mix:\n",
    "                    loss = mixup_criterion(self.criterion, outputs, labels_a, labels_b, lam)\n",
    "                else:\n",
    "                    loss = self.criterion(outputs, labels)\n",
    "\n",
    "                    clean_running_loss += loss.item() * images.size(0)\n",
    "                    clean_samples += images.size(0)\n",
    "\n",
    "            loss = loss / self.accum_steps\n",
    "            \n",
    "            if self.use_amp:\n",
    "    \n",
    "                self.scaler.scale(loss).backward()\n",
    "\n",
    "                # grad clipping va fatto dopo unscale\n",
    "                if self.grad_clip and self.grad_clip > 0.:\n",
    "                    self.scaler.unscale_(self.optimizer)\n",
    "                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.grad_clip)\n",
    "\n",
    "                if (batch_idx % self.accum_steps) == (self.accum_steps - 1):\n",
    "                    self.scaler.step(self.optimizer)\n",
    "                    self.scaler.update()\n",
    "\n",
    "            else:\n",
    "                loss.backward()\n",
    "\n",
    "                if self.grad_clip and self.grad_clip > 0.:\n",
    "                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.grad_clip)\n",
    "                    \n",
    "                if (batch_idx % self.accum_steps) == (self.accum_steps - 1):    \n",
    "                    self.optimizer.step()\n",
    "\n",
    "            if self.use_ema:\n",
    "                self.ema.update()\n",
    "\n",
    "            running_loss += (loss.item() * self.accum_steps) * images.size(0)\n",
    "\n",
    "            if not will_mix:\n",
    "                preds = torch.argmax(outputs, dim=1)\n",
    "                correct += (preds == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "\n",
    "            pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "\n",
    "        epoch_loss = running_loss / len(self.train_loader.dataset)\n",
    "        clean_loss = clean_running_loss / clean_samples if clean_samples > 0 else None\n",
    "        epoch_acc = correct / total if total > 0 else None\n",
    "           \n",
    "        return epoch_loss, epoch_acc, clean_loss\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def validate(self, epoch: int) -> Dict[str, object]:\n",
    "\n",
    "        if self.use_ema and self.ema is not None:\n",
    "            self.ema.apply_shadow()\n",
    "\n",
    "        self.model.eval()\n",
    "        running_loss = 0.0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        pbar = tqdm(self.val_loader, desc=f'Epoch {epoch} [VAL]')\n",
    "        for images, labels in pbar:\n",
    "            images = images.to(self.device, non_blocking=True)\n",
    "            labels = labels.to(self.device, non_blocking=True)\n",
    "            \n",
    "            try:\n",
    "                outputs = self.model(images)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Batch saltato per errore: {e}\")\n",
    "                continue\n",
    "            \n",
    "            running_loss += loss.item() * images.size(0)\n",
    "\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            all_preds.extend(preds.detach().cpu().numpy())\n",
    "            all_labels.extend(labels.detach().cpu().numpy())\n",
    "\n",
    "        # ripristina pesi normali dopo la validazione\n",
    "        if self.use_ema and self.ema is not None:\n",
    "            self.ema.restore()\n",
    "        \n",
    "        val_loss = running_loss / len(self.val_loader.dataset)\n",
    "        val_acc = accuracy_score(all_labels, all_preds)\n",
    "        val_balanced_acc = balanced_accuracy_score(all_labels, all_preds)\n",
    "        val_f1_macro = f1_score(all_labels, all_preds, average='macro')\n",
    "        val_f1_weighted = f1_score(all_labels, all_preds, average='weighted')\n",
    "        \n",
    "        return {\n",
    "            'loss': val_loss,\n",
    "            'accuracy': val_acc,\n",
    "            'balanced_accuracy': val_balanced_acc,\n",
    "            'f1_macro': val_f1_macro,\n",
    "            'f1_weighted': val_f1_weighted,\n",
    "            'preds': all_preds,\n",
    "            'labels': all_labels\n",
    "        }\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def test(self, test_loader, use_tta=True):\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"üß™ TEST SET EVALUATION\")\n",
    "        print(\"=\"*60)\n",
    "\n",
    "        # Usa EMA per il test (molto consigliato)\n",
    "        if self.use_ema and self.ema is not None:\n",
    "            self.ema.apply_shadow()\n",
    "\n",
    "        self.model.eval()\n",
    "        \n",
    "        all_preds = [] \n",
    "        all_labels = []\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for images, labels in tqdm(test_loader, desc='[TEST]'):\n",
    "            images = images.to(self.device, non_blocking=True)\n",
    "            labels = labels.to(self.device, non_blocking=True)\n",
    "\n",
    "            if use_tta:\n",
    "                with torch.cuda.amp.autocast(enabled=self.use_amp):\n",
    "                    outputs = self.tta_forward(images)\n",
    "            else:\n",
    "                with torch.cuda.amp.autocast(enabled=self.use_amp):\n",
    "                    outputs = self.model(images)\n",
    "\n",
    "            loss = self.criterion(outputs, labels)\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "    \n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        test_loss = running_loss / len(test_loader.dataset)\n",
    "        test_acc = accuracy_score(all_labels, all_preds)\n",
    "        test_balanced_acc = balanced_accuracy_score(all_labels, all_preds)\n",
    "        test_f1_macro = f1_score(all_labels, all_preds, average='macro')\n",
    "\n",
    "        print(f\"\\nTest Loss: {test_loss:.4f}\")\n",
    "        print(f\"Test Acc:  {test_acc:.4f}\")\n",
    "        print(f\"Balanced:  {test_balanced_acc:.4f}\")\n",
    "        print(f\"F1 macro:  {test_f1_macro:.4f}\")\n",
    "\n",
    "        # Confusion matrix test\n",
    "        self.plot_confusion_matrix(all_labels, all_preds, epoch=\"TEST\")\n",
    "\n",
    "        self.history['test_loss'] = test_loss\n",
    "        self.history['test_acc'] = test_acc\n",
    "        self.history['test_balanced_acc'] = test_balanced_acc\n",
    "        self.history['test_f1'] = test_f1_macro\n",
    "        self.save_history()\n",
    "\n",
    "        return {\n",
    "            'loss': test_loss,\n",
    "            'accuracy': test_acc,\n",
    "            'balanced_accuracy': test_balanced_acc,\n",
    "            'f1_macro': test_f1_macro,\n",
    "            'preds': all_preds,\n",
    "            'labels': all_labels\n",
    "        }\n",
    "    \n",
    "    def train(\n",
    "        self,\n",
    "        num_epochs: int = 50,\n",
    "        early_stopping_patience: int = 15,\n",
    "        freeze_epochs: int = 5,\n",
    "        start_epoch=1,  \n",
    "    ):\n",
    "        print(f\"Epochs: {num_epochs}, Early Stop Patience: {early_stopping_patience}\")\n",
    "        print(f\"Freeze Backbone: {freeze_epochs} epochs\")\n",
    "        print(f\"Mixup: {self.use_mixup}, CutMix: {self.use_cutmix}, mix_prob: {self.mixup_prob}\")\n",
    "        print(f\"Gradient Clipping: {self.grad_clip}\")\n",
    "        \n",
    "        # Freeze iniziale (solo se partiamo da epoch 1)\n",
    "        if freeze_epochs > 0 and start_epoch == 1:\n",
    "            self.model.freeze_backbone()\n",
    "        \n",
    "        patience_counter = 0\n",
    "        start_time = time.time()\n",
    "        \n",
    "        for epoch in range(start_epoch, num_epochs + 1):\n",
    "            \n",
    "            if freeze_epochs > 0 and epoch == freeze_epochs + 1 and start_epoch == 1:\n",
    "                if hasattr(self.model, 'unfreeze_backbone'):\n",
    "                    self.model.unfreeze_backbone()\n",
    "                else:\n",
    "                    for p in self.model.parameters():\n",
    "                        p.requires_grad = True\n",
    "\n",
    "                backbone_params = []\n",
    "                head_params = []\n",
    "\n",
    "                for name, p in self.model.named_parameters():\n",
    "                    if \"classifier\" in name:\n",
    "                        head_params.append(p)\n",
    "                    else:\n",
    "                        backbone_params.append(p)\n",
    "\n",
    "                current_lr = self.optimizer.param_groups[0]['lr'] * 0.1\n",
    "\n",
    "                self.optimizer = optim.AdamW([\n",
    "                    {'params': self.model.backbone.parameters(), 'lr': current_lr},\n",
    "                    {'params': self.model.classifier.parameters(), 'lr': current_lr}\n",
    "                ],\n",
    "                    weight_decay=self.config.get('weight_decay', 0.05)\n",
    "                )\n",
    "\n",
    "                self.scheduler = CosineAnnealingWarmRestarts(\n",
    "                    self.optimizer,\n",
    "                    T_0=10,\n",
    "                    T_mult=2,\n",
    "                    eta_min=1e-6,\n",
    "                    last_epoch=-1\n",
    "                )\n",
    "\n",
    "                # reset scaler per sicurezza\n",
    "                self.scaler = GradScaler(enabled=self.use_amp)\n",
    "                print(\"Backbone sbloccato + optimizer/scheduler aggiornati\")\n",
    "            \n",
    "            train_loss, train_acc, clean_loss = self.train_epoch(epoch)\n",
    "            val_metrics = self.validate(epoch)\n",
    "\n",
    "\n",
    "            if self.scheduler is not None:\n",
    "                self.scheduler.step()\n",
    "\n",
    "            \n",
    "            current_lrs = [param_group['lr'] for param_group in self.optimizer.param_groups]\n",
    "\n",
    "            self.history['lr'].append(current_lrs)\n",
    "            \n",
    "            self.history['train_loss'].append(train_loss)\n",
    "            self.history.setdefault('clean_train_loss', []).append(clean_loss)\n",
    "            self.history['val_loss'].append(val_metrics['loss'])\n",
    "            \n",
    "            self.history['train_acc'].append(train_acc)\n",
    "            self.history['val_acc'].append(val_metrics['accuracy'])\n",
    "            \n",
    "            self.history['val_balanced_acc'].append(val_metrics['balanced_accuracy'])\n",
    "            self.history['val_f1'].append(val_metrics['f1_macro'])\n",
    "            \n",
    "            # Print metriche\n",
    "            overfit_gap = (train_acc - val_metrics['accuracy'])\n",
    "\n",
    "            print(f\"Epoch {epoch}/{num_epochs}\")\n",
    "\n",
    "            if train_acc is not None:\n",
    "                print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
    "            else:\n",
    "                print(f\"Train Loss: {train_loss:.4f} | Train Acc: n/a (mixing used)\")\n",
    "\n",
    "            print(f\"Val Loss:   {val_metrics['loss']:.4f} | Val Acc:   {val_metrics['accuracy']:.4f}\")\n",
    "            print(f\"Balanced Acc: {val_metrics['balanced_accuracy']:.4f} | F1: {val_metrics['f1_macro']:.4f}\")\n",
    "            print(f\"Overfit Gap: {overfit_gap:.4f} ({overfit_gap*100:.1f}%)\")\n",
    "            print(f\"LR: {current_lrs[0]:.8e}\")\n",
    "            \n",
    "            is_best = val_metrics['balanced_accuracy'] > self.best_balanced_acc\n",
    "            if is_best:\n",
    "                self.best_balanced_acc = val_metrics['balanced_accuracy']\n",
    "                self.best_val_acc = val_metrics['accuracy']\n",
    "                patience_counter = 0\n",
    "                print(f\"Best model! Balanced Acc: {self.best_balanced_acc:.4f}\")\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "            \n",
    "            self._patience_counter = patience_counter\n",
    "            self.save_checkpoint(epoch, is_best=is_best)\n",
    "            \n",
    "            self.save_history()\n",
    "            self.plot_training_curves()\n",
    "            \n",
    "            # Salva confusion matrix se best\n",
    "            if is_best:\n",
    "                self.plot_confusion_matrix(val_metrics['labels'], val_metrics['preds'], epoch)\n",
    "            \n",
    "            if patience_counter >= early_stopping_patience:\n",
    "                print(f\"\\n Early stopping at epoch {epoch}\")\n",
    "                break\n",
    "            \n",
    "            print(f\"Patience: {patience_counter}/{early_stopping_patience}\")\n",
    "        \n",
    "        total_time = time.time() - start_time\n",
    "        print(f\"\\n COMPLETATO in {total_time/60:.2f} min\")\n",
    "        print(f\"Best Val Acc: {self.best_val_acc:.4f}, Balanced: {self.best_balanced_acc:.4f}\\n\")\n",
    "        \n",
    "        # Salvataggio finale\n",
    "        self.save_history()\n",
    "        \n",
    "        self.plot_training_curves()\n",
    "    \n",
    "    def save_checkpoint(self, epoch: int, is_best: bool = False):\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'scaler_state': self.scaler.state_dict() if self.use_amp else None,\n",
    "            'ema_state': (self.ema.shadow if (self.use_ema and self.ema is not None) else None),\n",
    "            'scheduler_state_dict': self.scheduler.state_dict() if self.scheduler else None,\n",
    "            'best_val_acc': self.best_val_acc,\n",
    "            'best_balanced_acc': self.best_balanced_acc,\n",
    "            'history': self.history,\n",
    "            'patience_counter': 0 if is_best else getattr(self, '_patience_counter', 0),\n",
    "            'config': self.config\n",
    "        }\n",
    "        \n",
    "        # Salva SEMPRE ultimo checkpoint (per resume)\n",
    "        last_checkpoint_path = self.save_dir / 'last_checkpoint.pth'\n",
    "        torch.save(checkpoint, last_checkpoint_path)\n",
    "        print(f\"Checkpoint salvato: epoch {epoch}\")\n",
    "        \n",
    "        # Salva best model separatamente\n",
    "        if is_best:\n",
    "            best_checkpoint_path = self.save_dir / 'best_model.pth'\n",
    "            torch.save(checkpoint, best_checkpoint_path)\n",
    "            print(f\"Best model aggiornato!\")\n",
    "        \n",
    "        # Salva backup ogni 5 epochs\n",
    "        if epoch % 5 == 0:\n",
    "            backup_path = self.save_dir / f'checkpoint_epoch_{epoch}.pth'\n",
    "            torch.save(checkpoint, backup_path)\n",
    "            print(f\"Backup salvato: epoch {epoch}\")\n",
    "    \n",
    "    def save_history(self):\n",
    "        with open(self.log_dir / 'history.json', 'w') as f:\n",
    "            json.dump(self.history, f, indent=2)\n",
    "    \n",
    "    def plot_confusion_matrix(self, labels, preds, epoch):\n",
    "\n",
    "        labels = np.array(labels)\n",
    "        preds = np.array(preds)\n",
    "\n",
    "        num_classes = self.model.classifier[-1].out_features\n",
    "    \n",
    "\n",
    "        if num_classes <= 20:\n",
    "            class_list = np.arange(num_classes)\n",
    "            cm = confusion_matrix(labels, preds, labels=class_list)\n",
    "\n",
    "            plt.figure(figsize=(12, 10))\n",
    "            sns.heatmap(cm, annot=False, fmt='d', cmap='Blues',\n",
    "                        xticklabels=class_list, yticklabels=class_list)\n",
    "        else:\n",
    "            unique, counts = np.unique(labels, return_counts=True)\n",
    "            top_classes = unique[np.argsort(counts)[-20:]]\n",
    "            \n",
    "            mask = np.isin(labels, top_classes)\n",
    "            cm = confusion_matrix(labels[mask], preds[mask], labels=top_classes)\n",
    "\n",
    "            cm_norm = cm.astype(np.float32) / cm.sum(axis=1, keepdims=True)\n",
    "            cm_norm = np.nan_to_num(cm_norm)\n",
    "        \n",
    "            plt.figure(figsize=(14, 12))\n",
    "            sns.heatmap(cm_norm, annot=False, fmt='d', cmap='Blues', vmin=0.0,\n",
    "    vmax=1.0, cbar=True,square=True, xticklabels=top_classes, yticklabels=top_classes)\n",
    "        \n",
    "        plt.title(f'Confusion Matrix - Epoch {epoch}')\n",
    "        plt.xlabel(\"Predicted\")\n",
    "        plt.ylabel(\"True\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(self.log_dir / f'confusion_matrix_ep{epoch}.png', dpi=300)\n",
    "        plt.close()\n",
    "    \n",
    "    def plot_training_curves(self):\n",
    "        if len(self.history['train_loss']) == 0:\n",
    "            return\n",
    "\n",
    "        clean_available = 'clean_train_loss' in self.history and any(\n",
    "        x is not None for x in self.history['clean_train_loss']\n",
    "        )\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "        epochs = range(1, len(self.history['train_loss']) + 1)\n",
    "\n",
    "        if clean_available:\n",
    "            axes[0, 0].plot(epochs, self.history['clean_train_loss'], \n",
    "                            label='Clean Train Loss (no mixup)', color='green')\n",
    "        \n",
    "        axes[0, 0].plot(epochs, self.history['val_loss'], label='Val', color=\"red\")\n",
    "        axes[0, 0].set_title('Loss'); axes[0, 0].legend()\n",
    "        \n",
    "        # Accuracy\n",
    "        axes[0, 1].plot(epochs, self.history['train_acc'], label='Train')\n",
    "        axes[0, 1].plot(epochs, self.history['val_acc'], label='Val')\n",
    "        axes[0, 1].set_title('Accuracy'); axes[0, 1].legend()\n",
    "        \n",
    "        # Overfitting gap\n",
    "        gap = [t - v for t, v in zip(self.history['train_acc'], self.history['val_acc'])]\n",
    "        axes[1, 0].plot(epochs, gap)\n",
    "        axes[1, 0].axhline(y=0, linestyle='--', alpha=0.5)\n",
    "        axes[1, 0].set_title('Overfitting Gap (Train - Val)')\n",
    "        \n",
    "        \n",
    "        # Metrics\n",
    "        axes[1, 1].plot(epochs, self.history['val_balanced_acc'], label='Balanced Acc Val')\n",
    "        axes[1, 1].plot(epochs, self.history['val_f1'], label='F1 Macro Val')\n",
    "        \n",
    "        axes[1, 1].set_title('Validation Metrics'); axes[1, 1].legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(self.log_dir / 'training_curves.png', dpi=300)\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "def get_dataloaders(\n",
    "    train_dir: str,\n",
    "    val_dir: str,\n",
    "    test_dir: str,\n",
    "    batch_size: int = 16,\n",
    "    img_size: int = 224,\n",
    "    augment_level: str = 'heavy',\n",
    "    use_weighted_sampling: bool = True,\n",
    "    num_workers: int = 4\n",
    "):\n",
    "    # transforms\n",
    "    mean = [0.485, 0.456, 0.406]\n",
    "    std  = [0.229, 0.224, 0.225]\n",
    "\n",
    "    if augment_level == 'heavy':\n",
    "        train_tf = T.Compose([\n",
    "            T.RandomResizedCrop(img_size, scale=(0.7, 1.0)),\n",
    "            T.RandomHorizontalFlip(),\n",
    "            T.RandomApply([T.ColorJitter(0.4,0.4,0.4,0.1)], p=0.8),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean=mean, std=std),\n",
    "            T.RandomErasing(p=0.15)\n",
    "        ])\n",
    "    else:\n",
    "        train_tf = T.Compose([\n",
    "            T.Resize(int(img_size*1.15)),\n",
    "            T.CenterCrop(img_size),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean=mean, std=std)\n",
    "        ])\n",
    "\n",
    "    val_tf = T.Compose([\n",
    "        T.Resize(int(img_size*1.1)),\n",
    "        T.CenterCrop(img_size),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=mean, std=std)\n",
    "    ])\n",
    "\n",
    "    train_ds = datasets.ImageFolder(train_dir, transform=train_tf)\n",
    "    val_ds = datasets.ImageFolder(val_dir, transform=val_tf)\n",
    "    test_ds = datasets.ImageFolder(test_dir, transform=val_tf)\n",
    "\n",
    "    \n",
    "    if use_weighted_sampling:\n",
    "        class_counts = np.bincount([y for _, y in train_ds.samples])\n",
    "        weights = 1.0 / (class_counts + 1e-6)\n",
    "        sample_weights = torch.tensor([weights[y] for _, y in train_ds.samples], dtype=torch.float)\n",
    "        \n",
    "        sampler = torch.utils.data.WeightedRandomSampler(sample_weights, len(sample_weights), replacement=True)\n",
    "        \n",
    "        train_loader = DataLoader(\n",
    "            train_ds, \n",
    "            batch_size=batch_size, \n",
    "            sampler=sampler, \n",
    "            num_workers=num_workers, \n",
    "            pin_memory=True,\n",
    "            drop_last=True\n",
    "        )    \n",
    "    else:\n",
    "        train_loader = DataLoader(\n",
    "            train_ds,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=num_workers,\n",
    "            pin_memory=True\n",
    "            )\n",
    "\n",
    "    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "    test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "    return train_loader, val_loader, test_loader, train_ds\n",
    "\n",
    "def main():\n",
    "    CONFIG = {\n",
    "        # Paths\n",
    "        'train_dir': path + '/classification/train',\n",
    "        'val_dir': path + '/classification/val',\n",
    "        'test_dir': path + '/classification/test',\n",
    "        'save_dir': '/kaggle/working/checkpoints',\n",
    "        'log_dir': '/kaggle/working/logs',\n",
    "        \n",
    "        # Model\n",
    "        'model_name': 'convnext_base.fb_in22k_ft_in1k',\n",
    "        'num_classes': 102,\n",
    "        'pretrained': True,\n",
    "        'dropout': 0.3,  \n",
    "        'drop_path_rate': 0.05,  \n",
    "        \n",
    "        # Training\n",
    "        'batch_size': 32,\n",
    "        'img_size': 224,\n",
    "        'num_epochs': 25,\n",
    "        'learning_rate': 1e-4,\n",
    "        'weight_decay': 0.05, \n",
    "        'augment_level': 'heavy',  \n",
    "        'use_weighted_sampling': True,\n",
    "        \n",
    "        # Regularization\n",
    "        'label_smoothing': 0.1,\n",
    "        'use_mixup': False,\n",
    "        'use_cutmix': False,\n",
    "        'mixup_prob': 0.6,\n",
    "        'mixup_alpha': 0.4,\n",
    "        'cutmix_alpha': 1.0,\n",
    "        'grad_clip': 1.0,\n",
    "        'freeze_backbone_epochs': 5,\n",
    "        'early_stopping_patience': 8,\n",
    "        \n",
    "        # Hardware\n",
    "        'num_workers': 4,\n",
    "        'device': 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    }\n",
    "    \n",
    "    print(\"‚öôÔ∏è  CONFIGURAZIONE\")\n",
    "    for key, value in CONFIG.items():\n",
    "        print(f\"{key:30s}: {value}\")\n",
    "    \n",
    "    # Data loaders\n",
    "    train_loader, val_loader, test_loader, train_ds = get_dataloaders(\n",
    "        train_dir=CONFIG['train_dir'],\n",
    "        val_dir=CONFIG['val_dir'],\n",
    "        test_dir=CONFIG['test_dir'],\n",
    "        batch_size=CONFIG['batch_size'],\n",
    "        img_size=CONFIG['img_size'],\n",
    "        augment_level=CONFIG['augment_level'],\n",
    "        use_weighted_sampling=CONFIG['use_weighted_sampling'],\n",
    "        num_workers=CONFIG['num_workers']\n",
    "    )\n",
    "    \n",
    "    # Model\n",
    "    model = IP102Classifier(\n",
    "        model_name=CONFIG['model_name'],\n",
    "        num_classes=CONFIG['num_classes'],\n",
    "        pretrained=CONFIG['pretrained'],\n",
    "        dropout=CONFIG['dropout'],\n",
    "        drop_path_rate=CONFIG['drop_path_rate']\n",
    "    )\n",
    "\n",
    "    if CONFIG[\"label_smoothing\"] > 0:\n",
    "        criterion = LabelSmoothingCrossEntropy(epsilon=CONFIG[\"label_smoothing\"])\n",
    "    else:\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    base_lr = CONFIG['learning_rate'] if CONFIG['learning_rate'] is not None else 1e-5\n",
    "    \n",
    "    optimizer = optim.AdamW([\n",
    "        {'params': model.backbone.parameters(), 'lr': base_lr},\n",
    "        {'params': model.classifier.parameters(), 'lr': base_lr}\n",
    "    ],\n",
    "        weight_decay=CONFIG['weight_decay']\n",
    "    )\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "        optimizer,\n",
    "        T_0=10,  \n",
    "        T_mult=2,      \n",
    "        eta_min=1e-6,\n",
    "        last_epoch=-1  \n",
    "    )\n",
    "\n",
    "    \n",
    "    # Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer,\n",
    "        scheduler=scheduler,\n",
    "        device=CONFIG['device'],\n",
    "        save_dir=CONFIG['save_dir'],\n",
    "        log_dir=CONFIG['log_dir'],\n",
    "        use_mixup=CONFIG['use_mixup'],\n",
    "        use_cutmix=CONFIG['use_cutmix'],\n",
    "        mixup_prob=CONFIG['mixup_prob'],\n",
    "        mixup_alpha=CONFIG['mixup_alpha'],\n",
    "        cutmix_alpha=CONFIG['cutmix_alpha'],\n",
    "        grad_clip=CONFIG['grad_clip'],\n",
    "        use_amp=True,\n",
    "        use_ema=True,\n",
    "        ema_decay=0.999,\n",
    "        config=CONFIG\n",
    "    )\n",
    "\n",
    "    last_ckpt = Path('/kaggle/input/convnext-15-epoch/checkpoints/last_checkpoint.pth')\n",
    "    \n",
    "\n",
    "    start_epoch = load_last_checkpoint(\n",
    "        trainer=trainer,\n",
    "        checkpoint_path=last_ckpt,\n",
    "        device=CONFIG['device'],\n",
    "        config = CONFIG\n",
    "    )\n",
    "\n",
    "    # Training\n",
    "    trainer.train(\n",
    "        num_epochs=CONFIG['num_epochs'],\n",
    "        early_stopping_patience=CONFIG['early_stopping_patience'],\n",
    "        freeze_epochs=CONFIG['freeze_backbone_epochs'],\n",
    "        start_epoch= start_epoch\n",
    "    )\n",
    "\n",
    "    # Test\n",
    "    results = trainer.test(test_loader, use_tta=True)\n",
    "    \n",
    "    print(\"\\n Training completato\")\n",
    "    print((results))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 1908726,
     "isSourceIdPinned": false,
     "sourceId": 3132677,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 9150886,
     "sourceId": 14333109,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31234,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 33906.333656,
   "end_time": "2026-02-01T07:50:35.714030",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2026-01-31T22:25:29.380374",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "3fe7f3adf64e43e0a6fffa335c5442ab": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "4be6650bf8954df7a9b587b2ea2004f7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_54d121277c8741518e71a315428f06c7",
       "placeholder": "‚Äã",
       "style": "IPY_MODEL_acef68136e9a4e74a651c68effdbe2c0",
       "tabbable": null,
       "tooltip": null,
       "value": "model.safetensors:‚Äá100%"
      }
     },
     "50fc2e65b1ae48ff8c601d1eb6090424": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_4be6650bf8954df7a9b587b2ea2004f7",
        "IPY_MODEL_69069b7e448647a2860f39b23d1aa0ce",
        "IPY_MODEL_ecccbc12d2c2425fadda8c2f31589fe6"
       ],
       "layout": "IPY_MODEL_733ecff36231479698d2ef7fbbff66eb",
       "tabbable": null,
       "tooltip": null
      }
     },
     "54d121277c8741518e71a315428f06c7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "5e453838692b4ff399f0644e67a0d99c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "69069b7e448647a2860f39b23d1aa0ce": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_3fe7f3adf64e43e0a6fffa335c5442ab",
       "max": 354400320,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_a10c2ac3049d4181a843da2309e23db6",
       "tabbable": null,
       "tooltip": null,
       "value": 354400320
      }
     },
     "733ecff36231479698d2ef7fbbff66eb": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "8310de6e79f34c8c99a9f38dd2ae3051": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "a10c2ac3049d4181a843da2309e23db6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "acef68136e9a4e74a651c68effdbe2c0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "ecccbc12d2c2425fadda8c2f31589fe6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_5e453838692b4ff399f0644e67a0d99c",
       "placeholder": "‚Äã",
       "style": "IPY_MODEL_8310de6e79f34c8c99a9f38dd2ae3051",
       "tabbable": null,
       "tooltip": null,
       "value": "‚Äá354M/354M‚Äá[00:01&lt;00:00,‚Äá390MB/s]"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
